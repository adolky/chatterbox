import random
import numpy as np
import torch
import gradio as gr
from chatterbox.tts import ChatterboxTTS
from chatterbox.mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES
import os
from datetime import datetime
import shutil
import re
import gc
import warnings
import logging
import time
import sys

# Configuration du logging production
LOG_DIR = "logs"
os.makedirs(LOG_DIR, exist_ok=True)

# Cr√©er un nom de fichier de log avec timestamp
log_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
log_file = os.path.join(LOG_DIR, f"gradio_app_{log_timestamp}.log")

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)
logger.info("="*60)
logger.info("CHATTERBOX TTS - MODE PRODUCTION")
logger.info(f"Fichier de log: {log_file}")
logger.info("="*60)

# D√©sactiver les warnings pour les autres biblioth√®ques
warnings.filterwarnings('ignore')
logging.getLogger('chatterbox').setLevel(logging.WARNING)
logging.getLogger('transformers').setLevel(logging.WARNING)
logging.getLogger('gradio').setLevel(logging.INFO)


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SAVED_VOICES_DIR = "voix_sauvegardees"

os.makedirs(SAVED_VOICES_DIR, exist_ok=True)


def set_seed(seed: int):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)


def load_model():
    logger.info("Chargement du mod√®le multilingue...")
    try:
        model = ChatterboxMultilingualTTS.from_pretrained(DEVICE)
        logger.info(f"Mod√®le charg√© avec succ√®s sur {DEVICE}")
        return model
    except Exception as e:
        logger.error(f"Erreur lors du chargement du mod√®le: {e}", exc_info=True)
        raise


def load_text_file(file):
    if file is None:
        return "", "‚ÑπÔ∏è Aucun texte"
    try:
        with open(file.name, 'r', encoding='utf-8') as f:
            content = f.read()
        return content, estimate_duration(content)
    except Exception as e:
        return f"Error loading file: {str(e)}", "‚ö†Ô∏è Erreur"


def estimate_duration(text):
    if not text or text.strip() == "":
        return "‚ÑπÔ∏è Aucun texte"
    words = len(text.split())
    minutes = words / 150
    hours = minutes / 60
    if hours >= 1:
        return f"üìä **Estimation** : ~{hours:.1f}h ({words:,} mots, {len(text):,} caract√®res)"
    elif minutes >= 1:
        return f"üìä **Estimation** : ~{minutes:.0f} min ({words:,} mots, {len(text):,} caract√®res)"
    else:
        return f"üìä **Estimation** : ~{minutes*60:.0f}s ({words:,} mots, {len(text):,} caract√®res)"


def get_saved_voices():
    voices = []
    if os.path.exists(SAVED_VOICES_DIR):
        for file in os.listdir(SAVED_VOICES_DIR):
            if file.endswith(('.wav', '.mp3', '.flac')):
                voices.append(file)
    return voices


def save_voice(audio_file, voice_name):
    if audio_file is None:
        return "‚ùå Aucun fichier audio √† sauvegarder", gr.update(choices=get_saved_voices())
    if not voice_name or voice_name.strip() == "":
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        voice_name = f"voix_{timestamp}"
    voice_name = voice_name.strip()
    ext = os.path.splitext(audio_file)[1]
    if not ext:
        ext = ".wav"
    filename = f"{voice_name}{ext}"
    save_path = os.path.join(SAVED_VOICES_DIR, filename)
    shutil.copy2(audio_file, save_path)
    return f"‚úÖ Voix sauvegard√©e : {filename}", gr.update(choices=get_saved_voices(), value=filename)


def load_saved_voice(voice_filename):
    if not voice_filename:
        return None
    voice_path = os.path.join(SAVED_VOICES_DIR, voice_filename)
    if os.path.exists(voice_path):
        return voice_path
    return None


def generate(model, text, language, audio_prompt_path, exaggeration, temperature, seed_num, cfgw, min_p, top_p, repetition_penalty, batch_size, max_tokens, progress=gr.Progress()):
    logger.info(f"Nouvelle g√©n√©ration - Langue: {language}, Longueur texte: {len(text)} caract√®res")
    
    if model is None:
        logger.info("Mod√®le non charg√©, chargement en cours...")
        model = ChatterboxMultilingualTTS.from_pretrained(DEVICE)
    if seed_num != 0:
        set_seed(int(seed_num))
    if not text or text.strip() == "":
        logger.warning("Tentative de g√©n√©ration avec texte vide")
        raise gr.Error("‚ö†Ô∏è Veuillez entrer du texte ou charger un fichier !")
    
    # D√âSACTIVER la d√©tection de r√©p√©tition pour TOUTES les langues = qualit√© + vitesse
    # C'est la cl√© pour √©viter la troncature pr√©matur√©e du texte
    use_analyzer = False
    
    # Optimisations sp√©cifiques par langue pour maximiser vitesse ET qualit√©
    # NOUVELLE R√àGLE: Max tokens ‚â§ 650 pour garantir texte complet
    
    if language == "en":
        # üá¨üáß Anglais : mots courts, phon√©tique simple
        if max_tokens > 650:
            max_tokens = 650
            print(f"üá¨üáß Optimisation anglais - max_tokens ajust√© √† {max_tokens}")
        if batch_size < 400:
            batch_size = 400
            print(f"üá¨üáß Optimisation anglais - batch_size ajust√© √† {batch_size}")
            
    elif language == "fr":
        # üá´üá∑ Fran√ßais : liaisons, phon√©tique complexe
        if max_tokens > 650:
            max_tokens = 650
            print(f"üá´üá∑ Optimisation fran√ßais - max_tokens ajust√© √† {max_tokens}")
        if batch_size > 300 or batch_size < 250:
            batch_size = 280
            print(f"üá´üá∑ Optimisation fran√ßais - batch_size ajust√© √† {batch_size}")
            
    elif language in ["es", "it", "pt"]:
        # üá™üá∏üáÆüáπüáµüáπ Langues romanes
        if max_tokens > 650:
            max_tokens = 650
            print(f"üåç Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size < 350:
            batch_size = 350
            print(f"üåç Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
            
    elif language in ["de", "nl"]:
        # üá©üá™üá≥üá± Allemand/N√©erlandais : mots tr√®s longs
        if max_tokens > 650:
            max_tokens = 650
            print(f"üá©üá™ Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size > 320:
            batch_size = 320
            print(f"üá©üá™ Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
            
    elif language in ["ja", "zh", "ko"]:
        # üáØüáµüá®üá≥üá∞üá∑ Langues asiatiques : caract√®res complexes
        if max_tokens > 650:
            max_tokens = 650
            print(f"üåè Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size > 250:
            batch_size = 250
            print(f"üåè Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
            
    elif language in ["ar", "he"]:
        # üá∏üá¶üáÆüá± Langues s√©mitiques : √©criture RTL
        if max_tokens > 650:
            max_tokens = 650
            print(f"üïå Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size > 280:
            batch_size = 280
            print(f"üïå Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
            
    elif language in ["ru", "pl"]:
        # üá∑üá∫üáµüá± Langues slaves : phon√©tique complexe
        if max_tokens > 650:
            max_tokens = 650
            print(f"üá∑üá∫ Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size > 300:
            batch_size = 300
            print(f"üá∑üá∫ Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
            
    else:
        # üåç Autres langues : param√®tres par d√©faut
        if max_tokens > 650:
            max_tokens = 650
            print(f"üåç Optimisation {language.upper()} - max_tokens ajust√© √† {max_tokens}")
        if batch_size < 300:
            batch_size = 300
            print(f"üåç Optimisation {language.upper()} - batch_size ajust√© √† {batch_size}")
    
    print(f"üìù Text: {len(text)} chars | Language: {language} | Batch: {batch_size} | Max tokens: {max_tokens} | Analyzer: DISABLED")
    
    # Split long text into sentences to avoid memory issues
    # üéØ D√âCOUPAGE SIMPLE ET FIABLE PAR PHRASES COMPL√àTES
    # R√àGLE ABSOLUE: Ne JAMAIS couper avant un point (.)
    
    # √âtape 1: D√©couper en phrases compl√®tes
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    batches = []
    current_batch = []
    current_length = 0
    
    BATCH_LIMIT = int(batch_size)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        sentence_len = len(sentence)
        
        # Si ajouter cette phrase d√©passe la limite ET on a d√©j√† du contenu
        if current_length + sentence_len > BATCH_LIMIT and current_batch:
            # Sauvegarder le batch actuel (phrases compl√®tes)
            batches.append(" ".join(current_batch))
            # D√©marrer un nouveau batch avec cette phrase
            current_batch = [sentence]
            current_length = sentence_len
        else:
            # Ajouter la phrase au batch actuel
            current_batch.append(sentence)
            current_length += sentence_len
    
    # Ajouter le dernier batch
    if current_batch:
        batches.append(" ".join(current_batch))
    
    # üïê CALCUL DU TEMPS ESTIM√â
    # Estimation bas√©e sur le nombre de batches et la longueur totale
    total_chars = len(text)
    num_batches = len(batches)
    
    logger.info(f"Texte divis√© en {num_batches} batches ({total_chars} caract√®res)")
    
    # Temps par batch: ~75-90 secondes en moyenne (varie selon GPU)
    estimated_time_per_batch = 80  # secondes
    total_estimated_seconds = num_batches * estimated_time_per_batch
    
    # Convertir en minutes
    estimated_minutes = total_estimated_seconds / 60
    
    print(f"\n‚è±Ô∏è  ESTIMATION DE TEMPS:")
    print(f"   üìù Texte: {total_chars} caract√®res")
    print(f"   üì¶ Batches: {num_batches}")
    print(f"   ‚è∞ Temps estim√©: {estimated_minutes:.1f} minutes ({total_estimated_seconds//60:.0f}min {total_estimated_seconds%60:.0f}s)")
    print(f"   üöÄ D√©marrage de la g√©n√©ration...\n")
    
    # Initialiser la progression
    progress(0, desc=f"üéôÔ∏è Pr√©paration... {num_batches} batches √† g√©n√©rer")
    
    # Afficher les d√©tails des batches
    print(f"üì¶ Processing {len(batches)} batches")
    print(f"üìã Batch details:")
    for idx, batch in enumerate(batches):
        words = len(batch.split())
        sentences_count = batch.count('.') + batch.count('!') + batch.count('?')
        print(f"\n   Batch {idx+1}: {len(batch)} chars, ~{words} words, {sentences_count} sentences")
        print(f"      Starts: {batch[:70]}...")
        print(f"      Ends:   ...{batch[-70:]}")
        # V√©rifier que le batch se termine bien par . ! ou ?
        if batch and batch[-1] not in '.!?':
            print(f"      ‚ö†Ô∏è WARNING: Batch ne se termine PAS par un point!")
    
    print(f"\n")
    all_wavs = []
    
    # Utiliser ChatterboxMultilingualTTS pour TOUTES les langues (y compris anglais)
    # Param√®tres unifi√©s pour coh√©rence et qualit√©
    print(f"Using ChatterboxMultilingualTTS ({language}) - Unified settings")
    
    # üéØ Syst√®me de g√©n√©ration par groupe pour r√©duire le cleanup GPU
    # Cleanup seulement tous les 8 batches au lieu de 3
    BATCHES_PER_GROUP = 8  # Traiter 8 batches avant de cleanup (au lieu de 3)
    
    # Timer pour le temps r√©el
    import time
    start_time = time.time()
    
    for i, batch_text in enumerate(batches):
        # Mettre √† jour la progression avec d√©tails
        batch_progress = (i / len(batches))
        elapsed = time.time() - start_time
        elapsed_min = elapsed / 60
        
        # Calculer temps restant bas√© sur le progr√®s r√©el
        if i > 0:
            avg_time_per_batch = elapsed / i
            remaining_batches = len(batches) - i
            estimated_remaining = (avg_time_per_batch * remaining_batches) / 60
            progress_desc = (
                f"üéôÔ∏è Batch {i+1}/{len(batches)} | ‚è±Ô∏è {elapsed_min:.1f}min √©coul√©es | ~{estimated_remaining:.1f}min restantes\n"
                f"‚è∞ Temps estim√© total: {estimated_minutes:.1f} minutes"
            )
            progress(batch_progress, desc=progress_desc)
        else:
            progress_desc = (
                f"üéôÔ∏è Batch {i+1}/{len(batches)} | D√©marrage...\n"
                f"‚è∞ Temps estim√© total: {estimated_minutes:.1f} minutes"
            )
            progress(batch_progress, desc=progress_desc)
        
        print(f"üîä Batch {i+1}/{len(batches)}: {len(batch_text)} chars")
        print(f"   Preview: {batch_text[:80]}..." if len(batch_text) > 80 else f"   Text: {batch_text}")
        
        # Skip empty batches
        if not batch_text or not batch_text.strip():
            print(f"   ‚ö†Ô∏è Skipping empty batch")
            continue
        
        # üéØ AJUSTEMENT DYNAMIQUE DES TOKENS bas√© sur la longueur
        # Formule: Plus le batch est long, plus on donne de tokens
        # Range: 500 (batch court) ‚Üí 650 (batch long) - MAX 650
        
        batch_length_ratio = min(len(batch_text) / batch_size, 1.0)
        # Calculer tokens: 500 + (150 √ó ratio) = 500 √† 650
        batch_max_tokens = int(500 + (150 * batch_length_ratio))
        
        # Garantir entre 500 et 650
        batch_max_tokens = max(500, min(650, batch_max_tokens))
        
        print(f"   üéØ Tokens dynamiques: {batch_max_tokens} (longueur: {len(batch_text)}/{batch_size} = {batch_length_ratio*100:.0f}%)")
        
        try:
            wav = model.generate(
                language_id=language,
                text=batch_text,
                audio_prompt_path=audio_prompt_path,
                exaggeration=exaggeration,
                temperature=temperature,
                cfg_weight=cfgw,
                min_p=min_p,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                max_new_tokens=int(batch_max_tokens),
                use_alignment_analyzer=use_analyzer,  # DISABLED pour toutes les langues
            )
            
            # V√©rifier que l'audio a √©t√© g√©n√©r√©
            if wav is None or wav.numel() == 0:
                print(f"   ‚ùå WARNING: Batch {i+1} generated empty audio!")
                continue
                
            audio_duration = wav.shape[-1] / model.sr
            print(f"   ‚úÖ Generated {audio_duration:.2f}s of audio")
            all_wavs.append(wav.squeeze(0))
            
            # üßπ Cleanup GPU R√âDUIT: Seulement tous les 8 batches
            # Gain de vitesse significatif en r√©duisant les cleanups
            if (i + 1) % BATCHES_PER_GROUP == 0:
                print(f"   üßπ GPU cleanup (every {BATCHES_PER_GROUP} batches)")
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                gc.collect()
            
        except Exception as e:
            print(f"   ‚ùå ERROR generating batch {i+1}: {str(e)}")
            print(f"   Skipping this batch and continuing...")
            continue
    
    # üßπ Cleanup final apr√®s tous les batches
    progress(0.99, desc="üßπ Nettoyage final de la m√©moire GPU...")
    print(f"\nüßπ Final GPU cleanup")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    
    # Calculer le temps total √©coul√©
    total_elapsed = time.time() - start_time
    total_elapsed_min = total_elapsed / 60
    
    # V√©rifier qu'on a bien g√©n√©r√© tous les batches
    if len(all_wavs) == 0:
        raise gr.Error("‚ùå Aucun audio g√©n√©r√© ! V√©rifiez les param√®tres et les logs.")
    
    if len(all_wavs) < len(batches):
        print(f"‚ö†Ô∏è WARNING: Seulement {len(all_wavs)}/{len(batches)} batches g√©n√©r√©s avec succ√®s")
        print(f"   ‚Üí Certaines parties du texte peuvent manquer dans l'audio")
    
    progress(1.0, desc="‚úÖ Assemblage final de l'audio...")
    combined_wav = torch.cat(all_wavs, dim=-1)
    sr = model.sr
    
    total_duration = combined_wav.shape[-1] / sr
    expected_duration = len(text) / 15  # Approximation: 15 caract√®res par seconde
    
    # Afficher le r√©sum√© final avec temps r√©el vs estim√©
    print(f"\n{'='*60}")
    print(f"‚úÖ G√âN√âRATION TERMIN√âE !")
    print(f"{'='*60}")
    print(f"üìä Statistiques:")
    print(f"   ‚úÖ Batches g√©n√©r√©s: {len(all_wavs)}/{len(batches)}")
    print(f"   üéµ Audio g√©n√©r√©: {total_duration:.2f}s ({total_duration/60:.2f} min)")
    print(f"   ‚è±Ô∏è  Temps de g√©n√©ration: {total_elapsed_min:.2f} min")
    print(f"   ‚ö° Vitesse: {total_duration/60 / total_elapsed_min:.2f}x temps r√©el")
    print(f"   üìù Texte: {len(text)} caract√®res")
    
    # Comparer estimation vs r√©alit√©
    accuracy = (total_elapsed_min / estimated_minutes) * 100
    print(f"\nüéØ Pr√©cision de l'estimation:")
    print(f"   Estim√©: {estimated_minutes:.1f} min")
    print(f"   R√©el: {total_elapsed_min:.1f} min")
    print(f"   Pr√©cision: {accuracy:.0f}%")
    
    # Logger les statistiques
    logger.info(f"G√©n√©ration termin√©e - {len(all_wavs)} batches, {total_duration:.2f}s audio, {total_elapsed_min:.2f}min")
    print(f"{'='*60}\n")
    
    if total_duration < expected_duration * 0.7:
        print(f"‚ö†Ô∏è WARNING: L'audio semble trop court - v√©rifiez si du texte a √©t√© saut√©")
    
    return (sr, combined_wav.numpy())


with gr.Blocks(title="Chatterbox TTS - Longue Dur√©e Multilingue") as demo:
    model_state = gr.State(None)
    gr.Markdown("""
    # üéôÔ∏è Chatterbox TTS - G√©n√©rateur Audio Longue Dur√©e Multilingue
    ### G√©n√©rez des audios de 1-2h+ √† partir de texte ou de fichiers dans 24 langues
    """)
    with gr.Row():
        with gr.Column():
            gr.Markdown("### üìù Entr√©e de Texte")
            text_file = gr.File(
                label="üìÅ Option 1 : Charger un fichier texte (.txt, .md, etc.)",
                file_types=[".txt", ".md", ".text"],
                type="filepath"
            )
            text = gr.Textbox(
                value="Now let's make my mum's favourite. So three mars bars into the pan. Then we add the tuna and just stir for a bit, just let the chocolate and fish infuse. A sprinkle of olive oil and some tomato ketchup. Now smell that. Oh boy this is going to be incredible.",
                label="‚úçÔ∏è Option 2 : Saisir ou coller le texte directement",
                max_lines=50,
                lines=15,
                placeholder="Entrez votre texte ici (capacit√© illimit√©e - parfait pour 1-2h d'audio)..."
            )
            language = gr.Dropdown(
                choices=[(f"{name} ({code})", code) for code, name in sorted(SUPPORTED_LANGUAGES.items(), key=lambda x: x[1])],
                value="en",
                label="üåç Langue du texte",
                info="S√©lectionnez la langue du texte √† synth√©tiser"
            )
            duration_info = gr.Markdown("üìä **Estimation** : ~1 min (57 mots)")
            gr.Markdown("""
            üí° **Conseils pour les longs textes** :
            - Divisez en paragraphes naturels pour de meilleurs r√©sultats
            - ~18,000 mots = ~2h d'audio
            - ~9,000 mots = ~1h d'audio
            """)
            gr.Markdown("### üéµ Param√®tres Audio")
            with gr.Row():
                with gr.Column():
                    ref_wav = gr.Audio(
                        sources=["upload", "microphone"], 
                        type="filepath", 
                        label="üé§ Fichier Audio de R√©f√©rence (optionnel)", 
                        value=None
                    )
                with gr.Column():
                    saved_voices = gr.Dropdown(
                        choices=get_saved_voices(),
                        value=None,
                        label="üíæ Charger une voix sauvegard√©e",
                        info="S√©lectionnez une voix pr√©c√©demment sauvegard√©e",
                        interactive=True
                    )
            with gr.Row():
                voice_name = gr.Textbox(
                    label="üìù Nom de la voix (optionnel)",
                    placeholder="Ex: voix_homme_1, voix_femme_claire, etc.",
                    scale=3
                )
                save_btn = gr.Button("üíæ Sauvegarder cette voix", scale=1, size="sm")
            save_status = gr.Markdown("")
            exaggeration = gr.Slider(0.25, 2, step=.05, label="Exag√©ration (Neutre = 0.5)", value=.5)
            cfg_weight = gr.Slider(0.0, 1, step=.05, label="CFG/Rythme", value=0.5)
            with gr.Accordion("‚öôÔ∏è Options Avanc√©es", open=False):
                max_tokens = gr.Slider(
                    100, 1500, step=50, 
                    label="üöÄ Max Tokens", 
                    value=650,
                    info="Auto-ajust√©: 500-650 selon longueur batch | MAX 650 toutes langues | Cleanup GPU: 8 batches"
                )
                batch_size = gr.Slider(
                    200, 800, step=50, 
                    label="‚ö° Taille des lots (caract√®res)", 
                    value=400,
                    info="Auto-optimis√© par langue : üá¨üáß EN=400 | üá´üá∑ FR=280 | üá™üá∏ ES/IT/PT=350 | Plus grand = plus rapide"
                )
                seed_num = gr.Number(value=0, label="Graine al√©atoire (0 = al√©atoire)")
                temp = gr.Slider(0.05, 5, step=.05, label="Temp√©rature", value=.8)
                min_p = gr.Slider(0.00, 1.00, step=0.01, label="min_p (Recommand√© 0.02-0.1, 0 = d√©sactiv√©)", value=0.05)
                top_p = gr.Slider(0.00, 1.00, step=0.01, label="top_p (1.0 = d√©sactiv√© recommand√©)", value=1.00)
                repetition_penalty = gr.Slider(1.00, 2.00, step=0.01, label="P√©nalit√© de r√©p√©tition", value=1.15, info="1.15 recommand√© pour vitesse, 1.00 = d√©sactiv√©")
            run_btn = gr.Button("üé¨ G√©n√©rer l'Audio", variant="primary", size="lg")
        with gr.Column():
            gr.Markdown("### üîä Sortie Audio")
            audio_output = gr.Audio(label="Audio G√©n√©r√©")
            gr.Markdown("""
            ‚ÑπÔ∏è **Informations** :
            - La g√©n√©ration peut prendre du temps pour de longs textes
            - Pour les textes de 1-2h, cela peut prendre plusieurs minutes
            - L'audio sera t√©l√©chargeable une fois la g√©n√©ration termin√©e
            
            üåç **Langues support√©es** : Arabe, Chinois, Danois, N√©erlandais, Anglais, Finnois, 
            Fran√ßais, Allemand, Grec, H√©breu, Hindi, Italien, Japonais, Cor√©en, Malais, 
            Norv√©gien, Polonais, Portugais, Russe, Espagnol, Swahili, Su√©dois, Turc
            """)
    demo.load(fn=load_model, inputs=[], outputs=model_state)
    text_file.change(fn=load_text_file, inputs=[text_file], outputs=[text, duration_info])
    text.change(fn=estimate_duration, inputs=[text], outputs=duration_info)
    save_btn.click(fn=save_voice, inputs=[ref_wav, voice_name], outputs=[save_status, saved_voices])
    saved_voices.change(fn=load_saved_voice, inputs=[saved_voices], outputs=ref_wav)
    run_btn.click(
        fn=generate,
        inputs=[model_state, text, language, ref_wav, exaggeration, temp, seed_num, cfg_weight, min_p, top_p, repetition_penalty, batch_size, max_tokens],
        outputs=audio_output,
    )

if __name__ == "__main__":
    try:
        # Obtenir l'adresse IP locale
        import socket
        hostname = socket.gethostname()
        local_ip = socket.gethostbyname(hostname)
        
        logger.info("="*60)
        logger.info("D√©marrage de l'interface Gradio...")
        logger.info(f"Device: {DEVICE}")
        logger.info(f"Hostname: {hostname}")
        logger.info("="*60)
        logger.info("üì° ACC√àS √Ä L'APPLICATION:")
        logger.info(f"   üè† Localhost: http://localhost:7860")
        logger.info(f"   üåê R√©seau local: http://{local_ip}:7860")
        logger.info(f"   üíª Depuis autre PC: http://{local_ip}:7860")
        logger.info("="*60)
        
        print("\n" + "="*60)
        print("üöÄ CHATTERBOX TTS - MODE PRODUCTION")
        print("="*60)
        print(f"üì° Acc√®s local: http://localhost:7860")
        print(f"üåê Acc√®s r√©seau: http://{local_ip}:7860")
        print(f"üíª Depuis autre PC: http://{local_ip}:7860")
        print("="*60 + "\n")
        
        demo.queue(max_size=50, default_concurrency_limit=1).launch(
            share=True, 
            server_name="0.0.0.0", 
            server_port=7860, 
            inbrowser=True
        )
    except KeyboardInterrupt:
        logger.info("Arr√™t demand√© par l'utilisateur (Ctrl+C)")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Erreur fatale: {e}", exc_info=True)
        sys.exit(1)

